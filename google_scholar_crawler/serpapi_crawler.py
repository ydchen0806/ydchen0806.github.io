#!/usr/bin/env python3
"""
ä½¿ç”¨ SerpAPI è·å– Google Scholar ç»Ÿè®¡ä¿¡æ¯
SerpAPI æä¾›å…è´¹é¢åº¦ï¼ˆæ¯æœˆ100æ¬¡ï¼‰ï¼Œéå¸¸ç¨³å®šå¯é 

åŠŸèƒ½ï¼š
1. ä» SerpAPI è·å–çœŸå®çš„ Google Scholar æ•°æ®
2. è‡ªåŠ¨æ›´æ–°ä¿åº•æ•°æ®ï¼ˆå½“æˆåŠŸè·å–çœŸå®æ•°æ®æ—¶ï¼‰
3. ç”Ÿæˆ shields.io å¾½ç« æ•°æ®
4. ç”Ÿæˆå¼•ç”¨è¶‹åŠ¿ SVG å›¾
5. è·å–ä¸€ä½œè®ºæ–‡åˆ—è¡¨åŠå…¶å¼•ç”¨æ•°
6. ç­›é€‰é«˜å¼•ç”¨è®ºæ–‡ï¼ˆ>50ï¼‰ç”Ÿæˆå¾½ç« æ•°æ®
7. ç­›é€‰ç¬¦åˆæ¡ä»¶çš„ä¸€ä½œè®ºæ–‡ï¼Œæå–ç ”ç©¶æ–¹å‘å…³é”®è¯

æ³¨å†Œè·å– API Key: https://serpapi.com/ (å…è´¹æ³¨å†Œ)
"""
import json
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from collections import Counter

try:
    import requests
except ImportError:
    print("è¯·å®‰è£… requests: pip install requests")
    sys.exit(1)

# å¯¼å…¥ä¼šè®®/æœŸåˆŠç­‰çº§é…ç½®
try:
    from venue_config import is_qualified_venue, get_venue_level
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œå®šä¹‰ç®€å•ç‰ˆæœ¬
    def is_qualified_venue(venue_name):
        qualified = ['AAAI', 'NeurIPS', 'ICML', 'ICLR', 'CVPR', 'ICCV', 'ECCV', 
                     'ACL', 'EMNLP', 'IJCAI', 'MICCAI', 'TMI', 'TPAMI', 'TIP',
                     'TCSVT', 'JBHI', 'Medical Image Analysis']
        return any(q.upper() in venue_name.upper() for q in qualified)
    def get_venue_level(venue_name):
        return (None, None)


# ==================== ä¿åº•æ•°æ®é…ç½® ====================
# å½“ SerpAPI è¯·æ±‚å¤±è´¥æ—¶ä½¿ç”¨è¿™äº›å€¼
FALLBACK_DATA = {
    "name": "Yinda Chen",
    "citedby": 534,      # ä¿åº•å¼•ç”¨æ•°ï¼ˆä¼šè¢«è‡ªåŠ¨æ›´æ–°ï¼‰
    "hindex": 12,         # ä¿åº• h-indexï¼ˆä¼šè¢«è‡ªåŠ¨æ›´æ–°ï¼‰
    "i10index": 13,       # ä¿åº• i10-indexï¼ˆä¼šè¢«è‡ªåŠ¨æ›´æ–°ï¼‰
    "affiliation": "University of Science and Technology of China",
    "interests": ["Computer Vision", "Self-Supervised Learning", "Multimodal Learning"],
}

# ä½œè€…å§“åå˜ä½“ï¼ˆç”¨äºåŒ¹é…ä¸€ä½œï¼‰
AUTHOR_NAME_VARIANTS = [
    "Yinda Chen",
    "Y Chen",
    "YD Chen",
    "é™ˆèƒ¤è¾¾",
]
# =====================================================


def get_scholar_stats_serpapi(scholar_id: str, api_key: str) -> dict:
    """
    ä½¿ç”¨ SerpAPI è·å– Google Scholar ç»Ÿè®¡ä¿¡æ¯
    """
    print(f"[SerpAPI] æ­£åœ¨è·å–å­¦è€…ä¿¡æ¯: {scholar_id}")
    
    url = "https://serpapi.com/search.json"
    params = {
        "engine": "google_scholar_author",
        "author_id": scholar_id,
        "api_key": api_key,
        "hl": "en"
    }
    
    try:
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if "error" in data:
            print(f"[SerpAPI] API é”™è¯¯: {data['error']}")
            return None
        
        # æå–ä½œè€…ä¿¡æ¯
        author = data.get("author", {})
        cited_by = data.get("cited_by", {})
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        citations = cited_by.get("table", [])
        citations_all = 0
        citations_5y = 0
        h_index = 0
        h_index_5y = 0
        i10_index = 0
        i10_index_5y = 0
        
        for item in citations:
            if item.get("citations", {}).get("all") is not None:
                citations_all = item["citations"]["all"]
            if item.get("citations", {}).get("since_2020") is not None:
                citations_5y = item["citations"]["since_2020"]
            if item.get("h_index", {}).get("all") is not None:
                h_index = item["h_index"]["all"]
            if item.get("h_index", {}).get("since_2020") is not None:
                h_index_5y = item["h_index"]["since_2020"]
            if item.get("i10_index", {}).get("all") is not None:
                i10_index = item["i10_index"]["all"]
            if item.get("i10_index", {}).get("since_2020") is not None:
                i10_index_5y = item["i10_index"]["since_2020"]
        
        # è·å–å¼•ç”¨è¶‹åŠ¿å›¾æ•°æ®
        cited_by_graph = cited_by.get("graph", [])
        
        author_data = {
            "name": author.get("name", "Unknown"),
            "citedby": citations_all,
            "citedby5y": citations_5y,
            "hindex": h_index,
            "hindex5y": h_index_5y,
            "i10index": i10_index,
            "i10index5y": i10_index_5y,
            "affiliation": author.get("affiliations", ""),
            "interests": [interest.get("title", "") for interest in author.get("interests", [])],
            "thumbnail": author.get("thumbnail", ""),
            "citation_graph": cited_by_graph,  # å¼•ç”¨è¶‹åŠ¿æ•°æ®
            "updated": str(datetime.now()),
            "source": "serpapi",
            "publications": {}
        }
        
        print(f"[SerpAPI] æˆåŠŸè·å–æ•°æ®:")
        print(f"  å§“å: {author_data['name']}")
        print(f"  æ€»å¼•ç”¨æ•°: {author_data['citedby']}")
        print(f"  è¿‘5å¹´å¼•ç”¨: {author_data['citedby5y']}")
        print(f"  h-index: {author_data['hindex']}")
        print(f"  i10-index: {author_data['i10index']}")
        
        return author_data
        
    except requests.exceptions.RequestException as e:
        print(f"[SerpAPI] è¯·æ±‚å¤±è´¥: {e}")
        return None
    except json.JSONDecodeError as e:
        print(f"[SerpAPI] JSON è§£æå¤±è´¥: {e}")
        return None
    except Exception as e:
        print(f"[SerpAPI] æœªçŸ¥é”™è¯¯: {e}")
        return None


def get_articles_serpapi(scholar_id: str, api_key: str, num_articles: int = 100) -> list:
    """
    è·å–ä½œè€…çš„è®ºæ–‡åˆ—è¡¨
    """
    print(f"[SerpAPI] æ­£åœ¨è·å–è®ºæ–‡åˆ—è¡¨...")
    
    url = "https://serpapi.com/search.json"
    params = {
        "engine": "google_scholar_author",
        "author_id": scholar_id,
        "api_key": api_key,
        "hl": "en",
        "num": num_articles,
        "sort": "cited"  # æŒ‰å¼•ç”¨æ•°æ’åº
    }
    
    try:
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        if "error" in data:
            print(f"[SerpAPI] API é”™è¯¯: {data['error']}")
            return []
        
        articles = data.get("articles", [])
        print(f"[SerpAPI] è·å–åˆ° {len(articles)} ç¯‡è®ºæ–‡")
        return articles
        
    except Exception as e:
        print(f"[SerpAPI] è·å–è®ºæ–‡å¤±è´¥: {e}")
        return []


def is_first_author(authors_str: str, name_variants: list) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºä¸€ä½œï¼ˆåŒ…æ‹¬å…±åŒä¸€ä½œï¼‰
    """
    if not authors_str:
        return False
    
    # è·å–ç¬¬ä¸€ä½œè€…ï¼ˆé€—å·åˆ†éš”çš„ç¬¬ä¸€ä¸ªï¼‰
    first_author = authors_str.split(",")[0].strip()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•å§“åå˜ä½“
    for variant in name_variants:
        if variant.lower() in first_author.lower():
            return True
        # ä¹Ÿæ£€æŸ¥æ•´ä¸ªä½œè€…åˆ—è¡¨ä¸­æ˜¯å¦æœ‰æ ‡æ³¨å…±åŒä¸€ä½œï¼ˆ*ï¼‰
        if f"{variant}*" in authors_str or f"*{variant}" in authors_str:
            return True
    
    return False


def filter_first_author_papers(articles: list, name_variants: list) -> list:
    """
    ç­›é€‰ä¸€ä½œè®ºæ–‡
    """
    first_author_papers = []
    
    for article in articles:
        authors = article.get("authors", "")
        title = article.get("title", "")
        
        if is_first_author(authors, name_variants):
            first_author_papers.append({
                "title": title,
                "authors": authors,
                "year": article.get("year", ""),
                "citations": article.get("cited_by", {}).get("value", 0),
                "link": article.get("link", ""),
                "citation_id": article.get("citation_id", "")
            })
    
    print(f"[ç­›é€‰] æ‰¾åˆ° {len(first_author_papers)} ç¯‡ä¸€ä½œè®ºæ–‡")
    return first_author_papers


def extract_research_keywords(title: str) -> list:
    """
    ä»è®ºæ–‡æ ‡é¢˜ä¸­æå–ç ”ç©¶æ–¹å‘å…³é”®è¯
    """
    # å…³é”®è¯æ˜ å°„è¡¨
    keyword_patterns = {
        'Multimodal Learning': ['multimodal', 'multi-modal', 'vision-language', 'cross-modal'],
        'Self-Supervised Learning': ['self-supervised', 'self supervised', 'pretraining', 'pretrain', 'contrastive'],
        'Computer Vision': ['vision', 'visual', 'image segmentation', 'object detection', 'semantic'],
        'Image Compression': ['compression', 'coding', 'latent', 'entropy'],
        'Domain Adaptation': ['domain adaptation', 'unsupervised domain', 'transfer learning'],
        'Medical Imaging': ['medical', 'clinical', 'biomedical', 'health', 'CT', 'MRI', 'X-ray'],
        'Deep Learning': ['deep learning', 'neural network', 'transformer', 'attention'],
        'Generative Models': ['generative', 'generation', 'diffusion', 'GAN', 'VAE'],
        'Reinforcement Learning': ['reinforcement', 'RL', 'policy', 'reward'],
        'Natural Language Processing': ['language', 'NLP', 'text', 'speech', 'TTS'],
        'Representation Learning': ['representation', 'embedding', 'feature learning'],
        'Data-Centric AI': ['synthetic data', 'data generation', 'dataset', 'annotation'],
        'Embodied AI': ['embodied', 'robot', 'humanoid', 'manipulation'],
        '3D Vision': ['3D', 'point cloud', 'depth', 'volumetric', 'NeRF'],
    }
    
    title_lower = title.lower()
    found_keywords = []
    
    for keyword, patterns in keyword_patterns.items():
        for pattern in patterns:
            if pattern.lower() in title_lower:
                found_keywords.append(keyword)
                break
    
    return found_keywords


def filter_qualified_papers(papers: list, years_limit: int = 3) -> list:
    """
    ç­›é€‰ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡ï¼š
    - 3å¹´å†…
    - CCF BåŠä»¥ä¸Š æˆ– SCI äºŒåŒºåŠä»¥ä¸Š
    """
    current_year = datetime.now().year
    min_year = current_year - years_limit
    
    qualified = []
    for paper in papers:
        year = paper.get('year', '')
        try:
            paper_year = int(year) if year else 0
        except ValueError:
            paper_year = 0
        
        # æ£€æŸ¥å¹´ä»½
        if paper_year < min_year:
            continue
        
        # ä»æ ‡é¢˜æˆ–å…¶ä»–ä¿¡æ¯æ¨æ–­ä¼šè®®/æœŸåˆŠ
        # æ³¨æ„ï¼šSerpAPI è¿”å›çš„æ•°æ®å¯èƒ½æ²¡æœ‰ä¼šè®®åç§°ï¼Œéœ€è¦ä» pub.md åŒ¹é…
        title = paper.get('title', '')
        
        # æš‚æ—¶ä¿ç•™æ‰€æœ‰è¿‘3å¹´çš„è®ºæ–‡ï¼Œåç»­å¯ä»¥é€šè¿‡æ ‡é¢˜åŒ¹é… pub.md æ¥è¿‡æ»¤
        paper['keywords'] = extract_research_keywords(title)
        qualified.append(paper)
    
    return qualified


def generate_research_keywords_json(papers: list, output_path: str):
    """
    ä»è®ºæ–‡æ ‡é¢˜ç”Ÿæˆç ”ç©¶æ–¹å‘å…³é”®è¯ JSON
    """
    all_keywords = []
    
    for paper in papers:
        keywords = paper.get('keywords', [])
        if not keywords:
            keywords = extract_research_keywords(paper.get('title', ''))
        all_keywords.extend(keywords)
    
    # ç»Ÿè®¡å…³é”®è¯é¢‘ç‡
    keyword_counts = Counter(all_keywords)
    
    # è½¬æ¢ä¸ºæƒé‡ï¼ˆ1-5ï¼‰
    max_count = max(keyword_counts.values()) if keyword_counts else 1
    
    keywords_data = []
    for keyword, count in keyword_counts.most_common(12):  # å–å‰12ä¸ª
        weight = max(2, min(5, round((count / max_count) * 5)))
        keywords_data.append({
            'keyword': keyword,
            'count': count,
            'weight': weight
        })
    
    # ä¿å­˜ JSON
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(keywords_data, f, ensure_ascii=False, indent=2)
    
    print(f"[OK] ç ”ç©¶æ–¹å‘å…³é”®è¯å·²ä¿å­˜åˆ° {output_path}")
    return keywords_data


def filter_high_cited_papers(papers: list, min_citations: int = 50) -> list:
    """
    ç­›é€‰é«˜å¼•ç”¨è®ºæ–‡ï¼ˆ>= min_citationsï¼‰
    """
    high_cited = []
    for paper in papers:
        citations = paper.get('citations', 0)
        if citations >= min_citations:
            high_cited.append(paper)
    
    # æŒ‰å¼•ç”¨æ•°é™åºæ’åˆ—
    high_cited.sort(key=lambda x: x.get('citations', 0), reverse=True)
    
    print(f"[ç­›é€‰] æ‰¾åˆ° {len(high_cited)} ç¯‡é«˜å¼•ç”¨è®ºæ–‡ (>={min_citations})")
    return high_cited


def generate_citation_trend_svg(citation_graph: list, output_path: str, total_citations: int = None):
    """
    ç”Ÿæˆå¼•ç”¨è¶‹åŠ¿ SVG å›¾ï¼ˆæ˜¾ç¤ºç´¯è®¡å¼•ç”¨æ•°ï¼‰
    
    Args:
        citation_graph: SerpAPI è¿”å›çš„æ¯å¹´å¼•ç”¨æ•°æ®
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        total_citations: çœŸå®çš„æ€»å¼•ç”¨æ•°ï¼ˆç”¨äºæ ¡å‡†ï¼‰
    """
    if not citation_graph:
        print("[SVG] æ²¡æœ‰å¼•ç”¨è¶‹åŠ¿æ•°æ®")
        return
    
    # æå–å¹´ä»½å’Œæ¯å¹´æ–°å¢å¼•ç”¨æ•°
    years = [item.get("year", 0) for item in citation_graph]
    yearly_citations = [item.get("citations", 0) for item in citation_graph]
    
    if not years or not yearly_citations:
        return
    
    # è®¡ç®—ç´¯è®¡å¼•ç”¨æ•°
    cumulative_citations = []
    running_total = 0
    for cite in yearly_citations:
        running_total += cite
        cumulative_citations.append(running_total)
    
    # å¦‚æœæä¾›äº†çœŸå®æ€»å¼•ç”¨æ•°ï¼Œæ ¡å‡†æœ€åä¸€å¹´çš„å€¼
    if total_citations and cumulative_citations:
        # è®¡ç®—å·®å€¼ï¼Œåˆ†é…åˆ°å„å¹´ï¼ˆå‡è®¾æ—©æœŸæ•°æ®ç¼ºå¤±ï¼‰
        diff = total_citations - cumulative_citations[-1]
        if diff > 0:
            # å°†å·®å€¼åŠ åˆ°ç¬¬ä¸€å¹´ä¹‹å‰ï¼ˆä½œä¸ºåŸºç¡€å€¼ï¼‰
            cumulative_citations = [c + diff for c in cumulative_citations]
            print(f"[SVG] æ ¡å‡†ç´¯è®¡å¼•ç”¨æ•°: {cumulative_citations[-1] - diff} â†’ {total_citations}")
    
    # SVG å°ºå¯¸ï¼ˆåŠ å®½ä»¥æ˜¾ç¤ºæ›´å¤šæ ‡ç­¾ï¼‰
    width = 700
    height = 250
    padding_left = 60
    padding_right = 50
    padding_top = 50
    padding_bottom = 50
    chart_width = width - padding_left - padding_right
    chart_height = height - padding_top - padding_bottom
    
    # è®¡ç®—æ¯”ä¾‹
    max_citations = max(cumulative_citations) if cumulative_citations else 1
    x_step = chart_width / (len(years) - 1) if len(years) > 1 else chart_width
    y_scale = chart_height / max_citations if max_citations > 0 else 1
    
    # ç”ŸæˆæŠ˜çº¿ç‚¹
    points = []
    for i, (year, cite) in enumerate(zip(years, cumulative_citations)):
        x = padding_left + i * x_step
        y = height - padding_bottom - cite * y_scale
        points.append(f"{x},{y}")
    
    polyline_points = " ".join(points)
    
    # ç”Ÿæˆ SVG
    svg = f'''<?xml version="1.0" encoding="UTF-8"?>
<svg width="{width}" height="{height}" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <linearGradient id="gradient" x1="0%" y1="0%" x2="0%" y2="100%">
      <stop offset="0%" style="stop-color:#4285f4;stop-opacity:0.6" />
      <stop offset="100%" style="stop-color:#4285f4;stop-opacity:0.05" />
    </linearGradient>
  </defs>
  
  <!-- èƒŒæ™¯ -->
  <rect width="{width}" height="{height}" fill="#fafafa" rx="10"/>
  
  <!-- æ ‡é¢˜ -->
  <text x="{width/2}" y="30" text-anchor="middle" font-family="Arial, sans-serif" font-size="16" font-weight="bold" fill="#333">
    ğŸ“ˆ Cumulative Citations (Total: {cumulative_citations[-1]})
  </text>
  
  <!-- Yè½´æ ‡ç­¾ -->
  <g font-family="Arial, sans-serif" font-size="10" fill="#888">
'''
    
    # Yè½´åˆ»åº¦æ ‡ç­¾
    for i in range(5):
        y_pos = padding_top + i * chart_height / 4
        y_val = int(max_citations * (4 - i) / 4)
        svg += f'    <text x="{padding_left - 10}" y="{y_pos + 4}" text-anchor="end">{y_val}</text>\n'
        svg += f'    <line x1="{padding_left}" y1="{y_pos}" x2="{width - padding_right}" y2="{y_pos}" stroke="#e0e0e0" stroke-width="1"/>\n'
    
    svg += '  </g>\n\n'
    
    # æ·»åŠ å¡«å……åŒºåŸŸ
    first_x = padding_left
    last_x = padding_left + (len(years) - 1) * x_step
    fill_points = f"{first_x},{height-padding_bottom} " + polyline_points + f" {last_x},{height-padding_bottom}"
    svg += f'  <!-- å¡«å……åŒºåŸŸ -->\n'
    svg += f'  <polygon points="{fill_points}" fill="url(#gradient)"/>\n\n'
    
    # æ·»åŠ æŠ˜çº¿
    svg += f'  <!-- æŠ˜çº¿ -->\n'
    svg += f'  <polyline points="{polyline_points}" fill="none" stroke="#4285f4" stroke-width="3" stroke-linecap="round" stroke-linejoin="round"/>\n\n'
    
    # æ·»åŠ æ•°æ®ç‚¹å’Œæ ‡ç­¾
    svg += '  <!-- æ•°æ®ç‚¹å’Œæ ‡ç­¾ -->\n'
    for i, (year, cite) in enumerate(zip(years, cumulative_citations)):
        x = padding_left + i * x_step
        y = height - padding_bottom - cite * y_scale
        
        # æ•°æ®ç‚¹
        svg += f'  <circle cx="{x}" cy="{y}" r="5" fill="#4285f4" stroke="#fff" stroke-width="2"/>\n'
        
        # å¹´ä»½æ ‡ç­¾ï¼ˆXè½´ï¼‰
        if len(years) <= 12 or i % 2 == 0 or i == len(years) - 1:
            svg += f'  <text x="{x}" y="{height-padding_bottom+18}" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="#666">{year}</text>\n'
        
        # æ¯å¹´éƒ½æ ‡æ³¨ç´¯è®¡å¼•ç”¨æ•°ï¼ˆè°ƒæ•´ä½ç½®é¿å…é‡å ï¼‰
        # å¥‡æ•°å¹´ä»½æ ‡ç­¾åœ¨ä¸Šæ–¹ï¼Œå¶æ•°å¹´ä»½æ ‡ç­¾åœ¨ä¸‹æ–¹æ•°æ®ç‚¹æ—è¾¹
        if i % 2 == 0:
            label_y = y - 12 if y > padding_top + 30 else y + 20
        else:
            label_y = y + 18 if y < height - padding_bottom - 30 else y - 12
        
        svg += f'  <text x="{x}" y="{label_y}" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" font-weight="bold" fill="#4285f4">{cite}</text>\n'
    
    svg += '</svg>'
    
    # ä¿å­˜æ–‡ä»¶
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(svg)
    
    print(f"[SVG] å¼•ç”¨è¶‹åŠ¿å›¾å·²ä¿å­˜åˆ° {output_path}")


def update_fallback_in_script(citations: int, hindex: int, i10index: int):
    """
    è‡ªåŠ¨æ›´æ–°æœ¬è„šæœ¬ä¸­çš„ä¿åº•æ•°æ®
    å½“æˆåŠŸè·å–çœŸå®æ•°æ®æ—¶è°ƒç”¨æ­¤å‡½æ•°
    """
    script_path = Path(__file__)
    
    try:
        content = script_path.read_text(encoding="utf-8")
        
        # æ›´æ–° citedby
        content = re.sub(
            r'("citedby":\s*)(\d+)',
            f'\\g<1>{citations}',
            content
        )
        
        # æ›´æ–° hindex
        content = re.sub(
            r'("hindex":\s*)(\d+)',
            f'\\g<1>{hindex}',
            content
        )
        
        # æ›´æ–° i10index
        content = re.sub(
            r'("i10index":\s*)(\d+)',
            f'\\g<1>{i10index}',
            content
        )
        
        script_path.write_text(content, encoding="utf-8")
        print(f"[è‡ªåŠ¨æ›´æ–°] å·²æ›´æ–°ä¿åº•æ•°æ®: citations={citations}, h-index={hindex}, i10-index={i10index}")
        
    except Exception as e:
        print(f"[è‡ªåŠ¨æ›´æ–°] æ›´æ–°ä¿åº•æ•°æ®å¤±è´¥: {e}")


def get_fallback_data() -> dict:
    """è¿”å›ä¿åº•æ•°æ®"""
    print("[Fallback] ä½¿ç”¨ä¿åº•æ•°æ®...")
    data = FALLBACK_DATA.copy()
    data.update({
        "updated": str(datetime.now()),
        "source": "fallback",
        "publications": {}
    })
    return data


def main():
    """ä¸»å‡½æ•°"""
    # è·å–ç¯å¢ƒå˜é‡
    scholar_id = os.environ.get("GOOGLE_SCHOLAR_ID", "hCvlj5cAAAAJ")
    serpapi_key = os.environ.get("SERPAPI_KEY", "")
    
    print("=" * 60)
    print("Google Scholar ç»Ÿè®¡ä¿¡æ¯è·å–å·¥å…· (SerpAPI ç‰ˆæœ¬)")
    print("=" * 60)
    
    author_data = None
    first_author_papers = []
    
    # å°è¯•ä½¿ç”¨ SerpAPI
    if serpapi_key:
        print("\n[1] å°è¯•ä½¿ç”¨ SerpAPI...")
        author_data = get_scholar_stats_serpapi(scholar_id, serpapi_key)
        
        # å¦‚æœæˆåŠŸè·å–ï¼Œè‡ªåŠ¨æ›´æ–°ä¿åº•æ•°æ®
        if author_data and author_data.get("source") == "serpapi":
            update_fallback_in_script(
                author_data["citedby"],
                author_data["hindex"],
                author_data["i10index"]
            )
            
            # è·å–è®ºæ–‡åˆ—è¡¨
            print("\n[2] è·å–è®ºæ–‡åˆ—è¡¨...")
            articles = get_articles_serpapi(scholar_id, serpapi_key)
            
            # ç­›é€‰ä¸€ä½œè®ºæ–‡
            if articles:
                first_author_papers = filter_first_author_papers(articles, AUTHOR_NAME_VARIANTS)
                author_data["first_author_papers"] = first_author_papers
    else:
        print("\n[WARNING] SERPAPI_KEY æœªè®¾ç½®ï¼Œè·³è¿‡ SerpAPI")
        print("  è¯·åœ¨ GitHub Secrets ä¸­è®¾ç½® SERPAPI_KEY")
        print("  æ³¨å†Œè·å–å…è´¹ API Key: https://serpapi.com/")
    
    # å¦‚æœ SerpAPI å¤±è´¥ï¼Œä½¿ç”¨ä¿åº•æ•°æ®
    if not author_data:
        print("\n[3] ä½¿ç”¨ä¿åº•æ•°æ®...")
        author_data = get_fallback_data()
    
    # ä¿å­˜ç»“æœ
    print("\n" + "=" * 60)
    print("ä¿å­˜ç»“æœ...")
    
    os.makedirs("results", exist_ok=True)
    
    # ä¿å­˜å®Œæ•´æ•°æ®
    with open("results/gs_data.json", "w", encoding="utf-8") as f:
        json.dump(author_data, f, ensure_ascii=False, indent=2)
    print("[OK] å®Œæ•´æ•°æ®å·²ä¿å­˜åˆ° results/gs_data.json")
    
    # ä¿å­˜ shields.io æ ¼å¼æ•°æ® - å¼•ç”¨æ•°
    shieldio_data = {
        "schemaVersion": 1,
        "label": "citations",
        "message": str(author_data["citedby"]),
        "color": "blue"
    }
    with open("results/gs_data_shieldsio.json", "w", encoding="utf-8") as f:
        json.dump(shieldio_data, f, ensure_ascii=False, indent=2)
    print("[OK] å¼•ç”¨æ•°å¾½ç« å·²ä¿å­˜åˆ° results/gs_data_shieldsio.json")
    
    # ä¿å­˜ h-index å¾½ç« æ•°æ®
    hindex_data = {
        "schemaVersion": 1,
        "label": "h-index",
        "message": str(author_data["hindex"]),
        "color": "green"
    }
    with open("results/gs_hindex.json", "w", encoding="utf-8") as f:
        json.dump(hindex_data, f, ensure_ascii=False, indent=2)
    print("[OK] H-Index å¾½ç« å·²ä¿å­˜åˆ° results/gs_hindex.json")
    
    # ä¿å­˜ i10-index å¾½ç« æ•°æ®
    i10index_data = {
        "schemaVersion": 1,
        "label": "i10-index",
        "message": str(author_data["i10index"]),
        "color": "orange"
    }
    with open("results/gs_i10index.json", "w", encoding="utf-8") as f:
        json.dump(i10index_data, f, ensure_ascii=False, indent=2)
    print("[OK] I10-Index å¾½ç« å·²ä¿å­˜åˆ° results/gs_i10index.json")
    
    # ç”Ÿæˆå¼•ç”¨è¶‹åŠ¿ SVGï¼ˆä¼ å…¥çœŸå®æ€»å¼•ç”¨æ•°è¿›è¡Œæ ¡å‡†ï¼‰
    if author_data.get("citation_graph"):
        generate_citation_trend_svg(
            author_data["citation_graph"],
            "results/citation_trend.svg",
            total_citations=author_data.get("citedby", 0)
        )
    
    # ä¿å­˜ä¸€ä½œè®ºæ–‡åˆ—è¡¨
    if first_author_papers:
        with open("results/first_author_papers.json", "w", encoding="utf-8") as f:
            json.dump(first_author_papers, f, ensure_ascii=False, indent=2)
        print(f"[OK] ä¸€ä½œè®ºæ–‡åˆ—è¡¨å·²ä¿å­˜åˆ° results/first_author_papers.json ({len(first_author_papers)} ç¯‡)")
        
        # ç­›é€‰é«˜å¼•ç”¨è®ºæ–‡ (>=50)
        high_cited_papers = filter_high_cited_papers(first_author_papers, min_citations=50)
        if high_cited_papers:
            with open("results/high_cited_papers.json", "w", encoding="utf-8") as f:
                json.dump(high_cited_papers, f, ensure_ascii=False, indent=2)
            print(f"[OK] é«˜å¼•ç”¨è®ºæ–‡å·²ä¿å­˜åˆ° results/high_cited_papers.json ({len(high_cited_papers)} ç¯‡)")
        
        # ç­›é€‰è¿‘3å¹´ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡
        qualified_papers = filter_qualified_papers(first_author_papers, years_limit=3)
        if qualified_papers:
            with open("results/qualified_papers.json", "w", encoding="utf-8") as f:
                json.dump(qualified_papers, f, ensure_ascii=False, indent=2)
            print(f"[OK] ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡å·²ä¿å­˜åˆ° results/qualified_papers.json ({len(qualified_papers)} ç¯‡)")
            
            # ç”Ÿæˆç ”ç©¶æ–¹å‘å…³é”®è¯
            generate_research_keywords_json(qualified_papers, "results/research_keywords.json")
        
        # æ‰“å°ä¸€ä½œè®ºæ–‡æ‘˜è¦
        print("\nä¸€ä½œè®ºæ–‡åˆ—è¡¨:")
        for i, paper in enumerate(first_author_papers[:10], 1):  # åªæ˜¾ç¤ºå‰10ç¯‡
            print(f"  {i}. [{paper['citations']} å¼•ç”¨] {paper['title'][:60]}...")
    
    # è¾“å‡ºæ‘˜è¦
    print("\n" + "=" * 60)
    print("ç»Ÿè®¡æ‘˜è¦:")
    print(f"  å§“å: {author_data['name']}")
    print(f"  å¼•ç”¨æ•°: {author_data['citedby']}")
    print(f"  h-index: {author_data['hindex']}")
    print(f"  i10-index: {author_data['i10index']}")
    print(f"  ä¸€ä½œè®ºæ–‡æ•°: {len(first_author_papers)}")
    print(f"  æ•°æ®æº: {author_data['source']}")
    print(f"  æ›´æ–°æ—¶é—´: {author_data['updated']}")
    print("=" * 60)
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
